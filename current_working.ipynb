{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "oriented-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-motorcycle",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "comfortable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "#x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "#x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "x_train = x_train.reshape(60000, 784) / 255.0\n",
    "x_test = x_test.reshape(10000, 784) / 255.0\n",
    "\n",
    "y_train = y_train.reshape(60000,1)\n",
    "y_test = y_test.reshape(10000,1)\n",
    "#x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "unnecessary-closure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "colored-grave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tensorflow dataset\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "x_train = x_train.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "y_train = y_train.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-consultancy",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "canadian-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Deep_SVDD(Model):\n",
    "#     def __init__(self, hidden1, hidden2, latent, input_dim):\n",
    "#         super(Deep_SVDD, self).__init__()\n",
    "# #         self.encoder_layer1 = layers.Dense(hidden1, activation = 'relu')\n",
    "# #         self.encoder_layer2 = layers.Dense(hidden2, activation = 'relu')\n",
    "\n",
    "# #         self.latent = layers.Dense(latent, activation = 'relu')\n",
    "\n",
    "# #         self.decoder_layer1 = layers.Dense(hidden2, activation = 'relu')\n",
    "# #         self.decoder_layer2 = layers.Dense(hidden1, activation = 'relu')\n",
    "        \n",
    "#         self.encoder = tf.keras.Sequential([\n",
    "#             layers.Dense(hidden1, activation='relu'),\n",
    "#             layers.Dense(hidden2, activation='relu')\n",
    "#         ])\n",
    "        \n",
    "#         self.latent = layers.Dense(latent, activation = 'relu')\n",
    "        \n",
    "#         self.decoder = tf.keras.Sequential([\n",
    "#             layers.Dense(hidden2, activation='relu'),\n",
    "#             layers.Dense(hidden1, activation='linear'),\n",
    "#         ])\n",
    "        \n",
    "#         self.input_dim = input_dim\n",
    "#         self.input_layer = layers.Input(shape = input_dim)\n",
    "#         self.output_layer = self.call(self.input_layer)\n",
    "#         self.c = 0\n",
    "    \n",
    "#     def build_graph(self):\n",
    "#         inputs_ = layers.Input(shape = self.input_dim)\n",
    "#         return Model(inputs=inputs_, outputs=self.call(inputs_))\n",
    "#         #self._init_graph_network(inputs=self.input_layer,outputs=self.out)\n",
    "    \n",
    "#     def center_point(self):\n",
    "#         #self.c += self.latent\n",
    "#         pass\n",
    "#     #def build(self):\n",
    "#     #    self._is_graph_network = True\n",
    "#     #      self._init_graph_network(inputs=self.input_layer,outputs=self.output_layer)\n",
    "\n",
    "#     def call(self, input_data):\n",
    "#         x = self.encoder(input_data)\n",
    "#         x = self.latent(x)\n",
    "#         x = self.decoder(x)\n",
    "        \n",
    "#         #return Model(input_data, x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "historic-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pretrain_AutoEncoder(Model):\n",
    "    def __init__(self, hidden1, hidden2, latent, input_dim):\n",
    "        super(Pretrain_AutoEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.encoder_layer1 = layers.Dense(hidden1, activation = 'relu', name='encoder1')\n",
    "        self.encoder_layer2 = layers.Dense(hidden2, activation = 'relu', name='encoder2')\n",
    "\n",
    "        self.latent = layers.Dense(latent, activation = 'relu', name='latent')\n",
    "\n",
    "        self.decoder_layer1 = layers.Dense(hidden2, activation = 'relu', name='decoder1')\n",
    "        self.decoder_layer2 = layers.Dense(hidden1, activation = 'relu', name='decoder2')\n",
    "        self.outputs = layers.Dense(self.input_dim, activation = 'relu', name = 'outputs')\n",
    "#         self.encoder = tf.keras.Sequential([\n",
    "#             layers.Dense(hidden1, activation='relu'),\n",
    "#             layers.Dense(hidden2, activation='relu')\n",
    "#         ])\n",
    "        \n",
    "        #self.latent = layers.Dense(latent, activation = 'relu')\n",
    "        \n",
    "#         self.decoder = tf.keras.Sequential([\n",
    "#             layers.Dense(hidden2, activation='relu'),\n",
    "#             layers.Dense(hidden1, activation='linear'),\n",
    "#         ])\n",
    "        \n",
    "    \n",
    "    def build_graph(self):\n",
    "        inputs_ = layers.Input(shape=self.input_dim, name = 'inputs')\n",
    "        return Model(inputs=inputs_, outputs=self.call(inputs_))\n",
    "        #self._init_graph_network(inputs=self.input_layer,outputs=self.out)\n",
    "    \n",
    "    def call(self, input_data):\n",
    "#         x = self.encoder(input_data)\n",
    "#         x = self.latent(x)\n",
    "#         x = self.decoder(x)\n",
    "        x = self.encoder_layer1(input_data)\n",
    "        x = self.encoder_layer2(x)\n",
    "        x = self.latent(x)\n",
    "        x = self.decoder_layer1(x)\n",
    "        x = self.decoder_layer2(x)\n",
    "        x = self.outputs(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "rough-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSVDD(Model):\n",
    "    def __init__(self, hidden1, hidden2, latent, input_dim):\n",
    "        super(DeepSVDD, self).__init__()\n",
    "        self.encoder_layer1 = layers.Dense(hidden1, activation = 'relu', name='encoder1')\n",
    "        self.encoder_layer2 = layers.Dense(hidden2, activation = 'relu', name='encoder2')\n",
    "\n",
    "        self.latent = layers.Dense(latent, activation = 'relu', name='latent')\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "#         self.encoder = tf.keras.Sequential([\n",
    "#             layers.Dense(hidden1, activation='relu'),\n",
    "#             layers.Dense(hidden2, activation='relu')\n",
    "#         ])\n",
    "        \n",
    "#         self.latent = layers.Dense(latent, activation = 'relu')\n",
    "        \n",
    "#         self.input_dim = input_dim\n",
    "#         self.input_layer = layers.Input(shape = input_dim)\n",
    "#         self.output_layer = self.call(self.input_layer)\n",
    "    \n",
    "    def build_graph(self):\n",
    "        inputs_ = layers.Input(shape = self.input_dim, name = 'inputs')\n",
    "        return Model(inputs=inputs_, outputs=self.call(inputs_))\n",
    "        #self._init_graph_network(inputs=self.input_layer,outputs=self.out)\n",
    "    \n",
    "    def call(self, input_data):\n",
    "        x = self.encoder_layer1(input_data)\n",
    "        x = self.encoder_layer2(x)\n",
    "        x = self.latent(x)\n",
    "        #return Model(input_data, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "based-passion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.pretrain_weight\\\\'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_weight_path = ['.', 'pretrain_weight']\n",
    "pretrain_weight_path = '.pretrain_weight'\n",
    "os.path.join(pretrain_weight_path) + '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "delayed-dayton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HYKP\\\\PycharmProjects\\\\bistel\\\\Deep_SVDD\\\\pretrain_weight_path\\\\{epoch:03d}_{val_loss:.4f}.hdf5'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.path.abspath('.'), 'pretrain_weight_path', '{epoch:03d}_{val_loss:.4f}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "realistic-trunk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HYKP\\\\PycharmProjects\\\\bistel\\\\Deep_SVDD\\\\pretrain_weight_path'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.path.abspath('.'), 'pretrain_weight_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "equipped-population",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a: 0.12'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0.123213213213213213\n",
    "f\"a: {a:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "informational-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR_PATH = os.path.join(os.path.abspath('.'), 'pretrain_weight') # '.\\\\pretrain_weight\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "heated-vinyl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "a = ae\n",
    "if a:\n",
    "    print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "failing-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_DIR_PATH = os.path.join(os.path.abspath('.'), 'pretrain_weight') # '.\\\\pretrain_weight\\\\'\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, args, pretrain_model, svdd_model, train, test):\n",
    "        self.args = args\n",
    "        \n",
    "        if pretrain_model:\n",
    "            self.pretrain_model = pretrain_model\n",
    "        self.svdd_model = svdd_model\n",
    "        \n",
    "        self.x_train = train[0]\n",
    "        self.y_train = train[1]\n",
    "        \n",
    "        self.x_test = test[0]\n",
    "        self.y_test = test[1]\n",
    "    \n",
    "    def start_check(self):\n",
    "        if not os.path.exists('pretrain_weight'):\n",
    "            os.mkdir('pretrain_weight')\n",
    "        \n",
    "    \n",
    "    def pretrain_ae(self):\n",
    "        #ae = Pretrain_AutoEncoder(350, 50, 2, 784).build_graph()\n",
    "        ae = self.pretrain_model.build_graph()\n",
    "        loss_object=tf.keras.losses.MeanSquaredError() #SVDD\n",
    "        #loss_object = tf.keras.losses.SparseCategoricalCrossentropy() #mnist\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        #ae.compile(optimizer=optimizer, loss=loss_object, metrics=['mae']) #SVDD\n",
    "        ae.compile(optimizer=optimizer, loss=loss_object, metrics=['mae']) #mnist\n",
    "        hist = ae.fit(x=self.x_train, y=self.x_train, batch_size=self.args['batch'], validation_split=0.2, epochs=self.args['epochs'])\n",
    "        \n",
    "        encoder_part = Model(ae.get_layer('inputs').input, ae.get_layer('latent').output)\n",
    "        \n",
    "        encoder_part.save_weights(os.path.join(MODEL_SAVE_DIR_PATH, 'pretrain_ae.hdf5'))\n",
    "        \n",
    "        return hist\n",
    "    \n",
    "    def set_c(self, eps=0.1):\n",
    "        \"\"\"Initializing the center for the hypersphere\"\"\"\n",
    "        model = self.svdd_model.build_graph()\n",
    "        if self.args['pretrain'] == True:\n",
    "            model.load_weights(os.path.join(MODEL_SAVE_DIR_PATH, 'pretrain_ae.hdf5'))\n",
    "        \n",
    "        \n",
    "        #x_train = tf.data.Dataset.from_tensor_slices(self.x_train)\n",
    "        #x_train = x_train.shuffle(buffer_size=1024).batch(64)        \n",
    "        \n",
    "        z_list = []\n",
    "        z_list_temp = []\n",
    "        #new = np.array_split(x_train, 938, axis = 0)\n",
    "        #for k, x in enumerate(x_train):\n",
    "        #    z_list.append(model.predict(x))\n",
    "        \n",
    "        z = model.predict(self.x_train)        \n",
    "        #z_list = np.concatenate(z_list)\n",
    "        #c = z_list.mean(axis = 0)\n",
    "        c = z.mean()\n",
    "        \n",
    "        c[(abs(c) < eps) & (c < 0)] = -eps\n",
    "        c[(abs(c) < eps) & (c > 0)] = eps\n",
    "        \n",
    "        return c\n",
    "        \n",
    "        '''\n",
    "        for key, x in enumerate(self.x_train):\n",
    "            if key % self.args['batch'] == 0:\n",
    "                z_list.append(z_list_temp)\n",
    "                z_list_temp = []\n",
    "            else:\n",
    "                z = model.predict(np.expand_dims(x, axis = 0))\n",
    "                z_list.append(z)\n",
    "        print(len(z_list))\n",
    "        print(len(z_list[0]))\n",
    "        print(len(z_list[-1]))\n",
    "        '''\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            for x, _ in dataloader:\n",
    "                x = x.float().to(self.device)\n",
    "                z = model.encoder(x)\n",
    "                z_.append(z.detach())\n",
    "        z_ = torch.cat(z_)\n",
    "        c = torch.mean(z_, dim=0)\n",
    "        c[(abs(c) < eps) & (c < 0)] = -eps\n",
    "        c[(abs(c) < eps) & (c > 0)] = eps\n",
    "        return c\n",
    "        '''\n",
    "    \n",
    "    \n",
    "    def deep_svdd_train(self):\n",
    "        pass\n",
    "        \n",
    "            \n",
    "        \n",
    "  \n",
    "    def eval_step(self):\n",
    "        pass\n",
    "        #prediction = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "plastic-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svdd.build_graph()\n",
    "model.load_weights(os.path.join(MODEL_SAVE_DIR_PATH, 'pretrain_ae.hdf5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "married-reward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "l = len(x_train)\n",
    "math.ceil(60000/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "helpful-thomson",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 784)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for i in np.array_split(x_train, 938, axis = 0):\n",
    "    print(i.shape)\n",
    "    print(type(i))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "compound-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = model.predict(x_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "tutorial-portrait",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4077797"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "guided-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "appointed-chicken",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float32)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[(abs(c) < eps) & (c < 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "subsequent-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "current-space",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4077797"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "duplicate-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "c[(abs(c) < eps) & (c < 0)] = -eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "special-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array(c)\n",
    "c[(abs(c) < eps) & (c < 0)] = -eps\n",
    "c[(abs(c) < eps) & (c > 0)] = eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "waiting-alabama",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(4.4077797, dtype=float32)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "interpreted-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_list = []\n",
    "for k, x in enumerate(np.array_split(x_train, 938, axis = 0)):\n",
    "    z_list.append(model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "incomplete-outdoors",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-339-5288783f4071>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mz_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "z_list = np.concatenate(z_list)\n",
    "c = z_list.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "psychological-opening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "encoder1 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "encoder2 (Dense)             (None, 350)               175350    \n",
      "_________________________________________________________________\n",
      "latent (Dense)               (None, 2)                 702       \n",
      "=================================================================\n",
      "Total params: 568,552\n",
      "Trainable params: 568,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "latest-sunrise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(x_train[0], axis = 0).shape\n",
    "x_train[0][np.newaxis,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "annoying-lawrence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.387509, 4.905375]], dtype=float32)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "heavy-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, value in enumerate(x_train):\n",
    "    model.predict(np.expand_dims(value, axis = 0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "acute-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svdd = Pretrain_AutoEncoder(500, 350, 2, 784)\n",
    "#svdd.build((None,784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "touched-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "#svdd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "utility-chrome",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#os.environ[\"PATH\"] += os.pathsep + \"C:/Program Files/Graphviz/bin/\"\n",
    "#tf.keras.utils.plot_model(svdd.build_graph(), show_shapes=True, show_layer_names=True, rankdir='TB', dpi=100, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "later-workstation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#svdd.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "accomplished-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'epochs':2,\n",
    "       'batch': 64,\n",
    "       'pretrain':True,\n",
    "       'tensorflow_data_type':False}\n",
    "\n",
    "ae = Pretrain_AutoEncoder(500, 350, 2, 784)\n",
    "svdd = DeepSVDD(500, 350, 2, 784)\n",
    "\n",
    "trainer = Trainer(args, pretrain_model=ae, svdd_model=svdd, train=[x_train, x_train], test = [x_test, x_test])\n",
    "trainer.start_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "alleged-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist = trainer.pretrain_ae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "separated-brake",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float32' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-343-295e6d70f9f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-341-34ca0b37c4f2>\u001b[0m in \u001b[0;36mset_c\u001b[1;34m(self, eps)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float32' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "trainer.set_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-mercury",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
